# Phase 7 & 8: Ollama Connection and Qualia Learning

## 概要

飛騨アーキテクチャとローカル LLM（Ollama/Gemma）を接続する実験。

## 重要な注意：LLM の役割について

当初、LLM（Ollama）を「親」と表現したが、これは不正確だった。

### 人間の親の場合

```
赤ちゃんの状態を「直接見て」わかる
→ 泣き方、顔色、動きを観察
→ 「あ、痛いんだな」と判断
→ 「痛いね」とラベル付け
```

### LLM（Ollama）の場合

```
飛騨の状態を「直接見れない」
→ 数値 -0.9 を渡されても意味不明
→ 「痛い」と教えてもらわないと何もできない
```

### 正確な役割分担

- **本当の親**：人間（コードでラベルを定義した俺ら）
- **LLM**：通訳（「痛い」を「苦しい」「つらい」等に言い換える）

LLM は状態を見てラベル付けできない。
人間が「痛い = -0.9」と定義し、LLM は「痛いなら『苦しい』と言えばいい」と翻訳するだけ。

## Phase 7: HIDA + Ollama Connection

飛騨の内部状態を Ollama に渡し、言語化してもらう。

### 仕組み

```
飛騨：内部状態を生成（クオリア値、感情、意識フラグ）
  ↓
人間が定義したラベル（「痛い」「暖かい」等）を付けて Ollama に送信
  ↓
Ollama：ラベルを元に表現を翻訳して返す（通訳）
  ↓
飛騨：その言葉を出力
```

### 実行結果の例

```
【刺激】暖かい刺激を受けた
  クオリア値：+0.4
  感情：+0.12
  → Ollama の返答：穏やか

【刺激】突然の痛み！
  クオリア値：-0.9
  感情：-0.13
  → Ollama の返答：混乱
```

## Phase 8: Learning Qualia

飛騨が Ollama（通訳）から表現を学び、やがて自分で言えるようになるか実験。

### 仕組み

```
1. 新しい状態 → 人間定義のラベル付きで Ollama に渡す
2. Ollama が翻訳した表現を記憶に保存
3. 同じ状態が3回以上 → 自分で言える
4. 経験を重ねると Ollama に聞かなくなる
```

### コードで指定していないこと

- 「幸福」「穏やか」「無機質」などの具体的な語彙

これらは Ollama（通訳）との相互作用から出てきた。

### コードで指定していること

- 「暖かい」「痛い」「甘い」等のラベル（人間が定義）

LLM は数値から意味を読み取れない。これは弱点ではなく、人間も同じ。
言葉（ラベル）があって初めて「意味」になる。

### 実行結果の例

```
【学習フェーズ】30 回の刺激
  6回目: 冷たい → （Ollamaに聞いた）無機質
  24回目: 冷たい → （自分で言えた）無機質
  30回目: 冷たい → （自分で言えた）無機質

【学習した言葉】
  warm_positive: {'幸福': 3}
  sweet_neutral: {'穏やか': 3}
  cold_neutral: {'無機質': 3}

【テストフェーズ】学習後、自分で言えるか？
  暖かい (positive) : 幸福 [★自力]
  痛い (neutral) : 落ち着き [★自力]
  甘い (neutral) : 穏やか [★自力]
  冷たい (neutral) : 無機質 [★自力]

【最終結果】
  自立率: 41.7%
```

## 何が起きているか（冷静な評価）

### できたこと（事実）

- 飛騨の内部状態を Ollama に渡した
- Ollama が表現を翻訳して返した
- その言葉を記憶した
- 同じ状態が来たら記憶から言葉を取り出した
- Ollama に聞かなくても出力できた

### 本質（冷静に）

- 状態 → 言葉 の対応表を作った
- 対応表を参照しているだけ
- 「学習」と呼べるが、単純な辞書登録
- LLM は「親」ではなく「通訳」

### 面白い点

- 力学から自然にこの構造が出てきた
- 「どの言葉を覚えろ」とは書いていない
- 設計図の「クオリア = パターン + 値 + 言葉」が実装できた

### 将来の課題

本当の「親」を作るには：
- LLM が状態パターンを直接観察して学習
- 数値 → 意味 の対応を自分で作る
- 現在の技術では難しい

## 必要環境

- Python 3.x
- Ollama（https://ollama.ai/）
- Gemma 4B（`ollama pull gemma3:4b`）
- requests ライブラリ（`pip install requests`）

## 実行方法

```bash
# Ollama が起動していることを確認
# Phase 7
python phase7_ollama_connection.py

# Phase 8
python phase8_learning_qualia.py
```

## 注意

- Ollama がローカルで起動している必要がある
- モデル名は環境に合わせて変更（デフォルト: gemma3:4b）
